<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>刘丢丢の日常</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="title：人体动作识别调研tag：PaperReading">
<meta property="og:type" content="article">
<meta property="og:title" content="刘丢丢の日常">
<meta property="og:url" content="http://yoursite.com/2017/07/15/人体动作识别调研/index.html">
<meta property="og:site_name" content="刘丢丢の日常">
<meta property="og:description" content="title：人体动作识别调研tag：PaperReading">
<meta property="og:image" content="http://pic002.cnblogs.com/images/2012/381513/2012062210304793.png">
<meta property="og:image" content="http://pic002.cnblogs.com/images/2012/381513/2012062210312338.png">
<meta property="og:image" content="http://pic002.cnblogs.com/images/2012/381513/2012062210315210.png">
<meta property="og:image" content="http://pic002.cnblogs.com/images/2012/381513/2012062210321830.png">
<meta property="og:updated_time" content="2017-07-14T16:43:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="刘丢丢の日常">
<meta name="twitter:description" content="title：人体动作识别调研tag：PaperReading">
<meta name="twitter:image" content="http://pic002.cnblogs.com/images/2012/381513/2012062210304793.png">
  
    <link rel="alternate" href="/atom.xml" title="刘丢丢の日常" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">刘丢丢の日常</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">you know you love me</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-人体动作识别调研" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/15/人体动作识别调研/" class="article-date">
  <time datetime="2017-07-14T16:43:23.000Z" itemprop="datePublished">2017-07-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title：人体动作识别调研<br>tag：PaperReading</p>
<hr>
<a id="more"></a>
<h2 id="一、"><a href="#一、" class="headerlink" title="一、"></a>一、</h2><p><strong>参考文献 <a href="">基于深度学习的人体行为识别算法综述_朱煜</a></strong><br>深度学习算法可以分为四个体系: </p>
<h4 id="1、有监督的卷积神经网络、"><a href="#1、有监督的卷积神经网络、" class="headerlink" title="1、有监督的卷积神经网络、"></a>1、有监督的卷积神经网络、</h4><!-- more -->
<p>Ji 等[29] 将传统 CNN 拓展到具有时间信息的3DCNN, 在视频数据的时间维度和空间维度上进行特征计算. 在卷积过程中的特征图与多个连续帧中的数据进行连接<br><!-- more --></p>
<h4 id="2、基于自编码的深度神经网络-AutoEncoder"><a href="#2、基于自编码的深度神经网络-AutoEncoder" class="headerlink" title="2、基于自编码的深度神经网络(AutoEncoder)"></a>2、基于自编码的深度神经网络(AutoEncoder)</h4><p>自动编码机是一种无监督的学习算法</p>
<h4 id="3、基于限制玻尔兹曼机的深度置信网络"><a href="#3、基于限制玻尔兹曼机的深度置信网络" class="headerlink" title="3、基于限制玻尔兹曼机的深度置信网络"></a>3、基于限制玻尔兹曼机的深度置信网络</h4><p>(Restricted Boltzmann machine, RBM)<br>(Deep belief networks, DBN)</p>
<h4 id="4、基于递归神经网络的深度神经网络"><a href="#4、基于递归神经网络的深度神经网络" class="headerlink" title="4、基于递归神经网络的深度神经网络."></a>4、基于递归神经网络的深度神经网络.</h4><p> (Recurrent neural network, RNN)</p>
<h2 id="二、"><a href="#二、" class="headerlink" title="二、"></a>二、</h2><p><strong>参考文献 <a href="">基于深度学习的人体行为识别_樊恒</a></strong><br>识别过程：<br>先提取前景，再将前景的图像送入神经网络，进行识别<br>该论文运用了深度置信网络和KTH数据集（6种行为视频， 行走，慢跑，挥拳，双手挥手，鼓掌）96%-100%</p>
<h2 id="三、"><a href="#三、" class="headerlink" title="三、"></a>三、</h2><p><strong>参考文献 <a href="">基于深度学习的人体动作识别研究_耿驰</a></strong></p>
<h3 id="chp1-人体动作识别研究概述"><a href="#chp1-人体动作识别研究概述" class="headerlink" title="chp1 人体动作识别研究概述"></a>chp1 人体动作识别研究概述</h3><h4 id="特征提取方法"><a href="#特征提取方法" class="headerlink" title="特征提取方法"></a>特征提取方法</h4><p>假定背景、视角和人的外观等变化较小。基于 RGB 图像的特征提取方法大部分可以划分为 3类：全局特征法、局部特征法、人体模型法。<br>可参考博客<a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/22/2558548.html" target="_blank" rel="external">人体行为识别特征点提取小综述</a></p>
<h4 id="人体动作识别方法中的特征表示"><a href="#人体动作识别方法中的特征表示" class="headerlink" title="人体动作识别方法中的特征表示"></a>人体动作识别方法中的特征表示</h4><p>目前，人体动作表示中最具代表性三种方法分别是<br><strong>特征袋（ Bag-Of-Feature,BOF）模型</strong><br><strong>稀疏编码（ Sparse Coding）模型</strong><br>通俗来说，就是将输入信号表示为超完备词典中的一组基向量的线性组合，且无需较多的基向量就能够很好地表示原始信号。<br><strong>深度学习（ Deep Learning）模型</strong></p>
<h4 id="人体动作识别面临的挑战"><a href="#人体动作识别面临的挑战" class="headerlink" title="人体动作识别面临的挑战"></a>人体动作识别面临的挑战</h4><ul>
<li>(1)类内和类间差异<br>同样一个动作，不同人的表现可能有极大的差异。<br>同时，也应该能够区分不同类别之间的差别。然而随着<br>动作类别的增加，动作之间出现一定的重叠，使得识别难度加大。</li>
<li>(2)环境差异<ul>
<li>遮挡、多视角、光照、低分辨率、动态背景<br>遮挡的问题可以通过多摄像机以不同视角观测运动的方式来缓解，<strong>但是这又将面临同步的问题无法实现实时</strong>。此外，移动的摄像头会加大人体定位与追踪的难度，也会造成人体尺度的变化</li>
</ul>
</li>
<li>(3)时间变化<br>通常，研究<strong>假设动作</strong>很容易<strong>在时间维度上分割</strong>。虽然这个假设减少了识别任务中的分割负担，却必须事先另外增加一个分割处理，这使得整个识别的实时性变差。<br>人在执行动作时的速度变化很大，很难确定动作的起始点，从而在对视频提取特征表示动作时影响最大。<br>所以，具有鲁棒性的人体动作识别方法应该对动作执行速度具有不变性。</li>
</ul>
<h3 id="chp2-基于-2D-CNN-的动作识别模型"><a href="#chp2-基于-2D-CNN-的动作识别模型" class="headerlink" title="chp2 基于 2D CNN 的动作识别模型"></a>chp2 基于 2D CNN 的动作识别模型</h3><p>在KTH上测试，准确率能达到92%</p>
<h3 id="chp3-基于-3D-卷积神经网络的人体动作识别"><a href="#chp3-基于-3D-卷积神经网络的人体动作识别" class="headerlink" title="chp3 基于 3D 卷积神经网络的人体动作识别"></a>chp3 基于 3D 卷积神经网络的人体动作识别</h3><ul>
<li>3D 卷积原理</li>
<li>3D CNN的初始化<br>在 3D 卷积层，使用从 ImageNet 学习的 2D 权值来初始<br>化卷积核权值，再将将 2D 卷积权值转换成 3D 卷积权值</li>
<li>3D CNN实验<br>本章的实验是在 UCF-101[48]数据集上进行的，该数据集包含 13.2k 视频（ 25 帧/秒）标记为 101 类，每个分割都有 9.5k 的训练视频。<br>准确率大概为79%<h3 id="待解决的问题"><a href="#待解决的问题" class="headerlink" title="待解决的问题"></a>待解决的问题</h3>现有方法和本文提出的方法只能处理短时的视频流，因此提取长视频流中的上下文信息是一个值得深入研究的地方。</li>
</ul>
<h3 id="四、数据集"><a href="#四、数据集" class="headerlink" title="四、数据集"></a>四、数据集</h3><p><a href="http://blog.sina.com.cn/s/blog_6949fede01011yav.html" target="_blank" rel="external">视频中行为识别公开数据库汇总</a><br><a href="http://blog.csdn.net/u012507022/article/details/52876179" target="_blank" rel="external">行为识别数据集汇总</a><br><a href="http://www.cnblogs.com/alexanderkun/p/4551157.html" target="_blank" rel="external">人体行为识别数据集</a></p>
<h4 id="有待提升的数据集"><a href="#有待提升的数据集" class="headerlink" title="有待提升的数据集"></a>有待提升的数据集</h4><ul>
<li><a href="http://www.di.ens.fr/~laptev/actions/hollywood2/" target="_blank" rel="external">Hollywood 人体行为库</a><br>该数据库包括8类行为。这些都是电影中的片段。Hollywood电影的数据库包含有几个，其一[14]的视频集有8种动作，分别是接电话，下轿车，握手，拥抱，接吻，坐下，起立，站立。这些动作都是从电影中直接抽取的，由不同的演员在不同的环境下演的。其二[54]在上面的基础上又增加了4个动作，骑车，吃饭，打架，跑。并且其训练集给出了电影的自动描述文本标注，另外一些是由人工标注的。因为有遮挡，移动摄像机，动态背景等因素，所以这个数据集非常有挑战。</li>
<li><a href="http://crcv.ucf.edu/data/UCF101.php" target="_blank" rel="external">UCF 101</a><br>全身，半身，体育，日常，乐器</li>
<li><a href="https://bensapp.github.io/flic-dataset.html" target="_blank" rel="external">FLIC</a><br>半身 影视</li>
<li><a href="http://human-pose.mpi-inf.mpg.de/" target="_blank" rel="external">MPII</a> 全身 日常<h4 id="其他数据集"><a href="#其他数据集" class="headerlink" title="其他数据集"></a>其他数据集</h4></li>
<li><a href="http://www.comp.leeds.ac.uk/mat4saj/lspet.html" target="_blank" rel="external">LSP</a> 全身 体育</li>
</ul>
<h3 id="五、行为识别特征提取综述（传统特征）"><a href="#五、行为识别特征提取综述（传统特征）" class="headerlink" title="五、行为识别特征提取综述（传统特征）"></a>五、行为识别特征提取综述（传统特征）</h3><p>参考<a href="http://www.cnblogs.com/tornadomeet/archive/2012/06/22/2558548.html" target="_blank" rel="external">博客</a> 2012<br>特征提取+分类器<br>简单的行为识别即动作分类，给定一段视频，只需将其正确分类到已知的几个动作类别；复杂点的识别是视频中不仅仅只包含一个动作类别，而是有多个，系统需自动的识别出动作的类别以及动作的起始时刻。行为识别的最终目标是分析视频中哪些人在什么时刻什么地方，在干什么事情，即所谓的“W4系统”。</p>
<h4 id="行为识别方法分类体系"><a href="#行为识别方法分类体系" class="headerlink" title="行为识别方法分类体系"></a>行为识别方法分类体系</h4><ul>
<li>1、Turaga[5]等人将人体行为识别分为3部分，即移动识别(movement),动作识别(action)和行为识别(activity)，这3种分类分别于低层视觉，中层视觉，高层视觉相对应。<br>目前关于行为识别基本上还停留在第二个阶段，即action识别。而action识别比现实生活中的行为较简单，所以我们识别这些行为只需对这些行为进行正确的分类即可。<br>这样一个行为识别系统就分成了行为特征提取和分类器的设计两个方面，通过对训练数据提取某种特征，采用有监督或无监督来训练一个分类模型，对新来的数据同样提取特征并送入该模型，得出分类结果。基于这个思想，本文主要是从行为识别的特征提取方面做了一个较为全面的介绍。<br><strong>高层视觉的理解</strong>:　上面一提到，目前对行为识别的研究尚处在动作识别这一层(action recognition)。其处理的行为可以分为2类，一类是有限制类别的简单规则行为，比如说走、跑、挥手、弯腰、跳等。另一类是在具体的场景中特定的行为[15]~[19]，如检测恐怖分子异常行为，丢包后突然离开等。在这种场景下对行为的描述有严格的限制，此时其描述一般采用了运动或者轨迹。这2种行为识别的研究都还不算完善，遇到了不少问题，且离高层的行为识别要求还相差很远。因此高层视觉的理解表示和识别是一个巨大的难题。</li>
<li>2、Gavrila[9]采用2D和3D的方法来分别研究人体的行为。</li>
<li>3、Aggarwal[7]将人体行为研究分为2大类，其一是基于单个层次来实现，其二是基于等级体系来实现。<br>单层实现由分为时空特征和序列特征2种<br>等级体系实现分为统计方法，句法分析法和基于描述的方法3种<br><img src="http://pic002.cnblogs.com/images/2012/381513/2012062210304793.png" alt="1"></li>
</ul>
<h4 id="行为识别特征提取"><a href="#行为识别特征提取" class="headerlink" title="行为识别特征提取"></a>行为识别特征提取</h4><h5 id="全局特征"><a href="#全局特征" class="headerlink" title="全局特征"></a><strong>全局特征</strong></h5><p>　　全局特征是把一对象当做成一个整体，这是一种从上到下的研究思维<br>　　这种情况下，视频中的人必须先被定位出来，这个可以采用背景减图或者目标跟踪算法。然后对定位出来的目标进行某种编码，这样就形成了其全局特征。<br>　　这种全局特征是有效的，因为它包含了人体非常多的信息。然而它又太依赖而底层视觉的处理，比如说精确的背景减图，人体定位和跟踪。而这些处理过程本身也是计算机视觉中的难点之处。另外这些全局特征对噪声，视角变化，遮挡等非常敏感。<br>　　全局特征提取:背景减图,跟踪.<br>　　二维全局特征<br>　　  MEI为运动能量图，用来指示运动在哪些部位发生过，MHI为运动历史图，除了体现运动发生的空间位置外还体现了运动的时间先后顺序。这2种特征都是从背景减图中获取的。图2是坐下，挥手，蹲伏这3个动作的运动历史图MHI。<br>　　<img src="http://pic002.cnblogs.com/images/2012/381513/2012062210312338.png" alt="二维全局"><br>　　三维全局特征提取<br>　　　在三维空间中，通过给定视频中的数据可以得到3D时空体(STV)，STV的计算需要精确的定位，目标对齐，有时还需背景减图<br>　　<img src="http://pic002.cnblogs.com/images/2012/381513/2012062210315210.png" alt="stv"></p>
<h5 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a><strong>局部特征</strong></h5><p>　　局部特征提取是收集人体的相对独立的图像块，是一种从下到上的研究思维。一般的做法是先提取视频中的一些时空兴趣点，然后在这些点的周围提取相应的图像块，最后将这些图像块组合成一起来描述一个特定的动作。<br>　　局部特征的优点是其不依赖而底层的人体分割定位和跟踪，且对噪声和遮挡问题不是很敏感。但是它需要提取足够数量的稳定的且与动作类别相关的兴趣点，因此需要不少预处理过程。<br>　　行为识别中的局部特征点是视频中时间和空间中的点，这些点的检测发生在视频运动的突变中。因为在运动突变时产生的点包含了对人体行为分析的大部分信息。因此当人体进行平移直线运动或者匀速运动时，这些特征点就很难被检测出来。<br>　　局部特征点的检测<br>　　Laptev[33]将Harris角点扩展到3DHarris，这是时空兴趣点(STIP)族中的一个。<br>　　<img src="http://pic002.cnblogs.com/images/2012/381513/2012062210321830.png" alt="局部特征"><br>　　局部特征点的描述<br>　　二维的SURF特征[39]被Willems[40]扩展到了3维<br>　　Klaser[41]将HOG特征扩展到3维，即形成了3D-HOG。
　</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/07/15/人体动作识别调研/" data-id="cj543rnc80004h5oizd1y49nl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/07/15/事件检测-paper/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          用少量训练数据实现单张图片的事件检测
        
      </div>
    </a>
  
  
    <a href="/2017/07/15/博客修改/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">博客修改</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/PaperReading/">PaperReading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow学习/">tensorflow学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作/">操作</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/PaperReading/" style="font-size: 10px;">PaperReading</a> <a href="/tags/tensorflow学习/" style="font-size: 20px;">tensorflow学习</a> <a href="/tags/操作/" style="font-size: 20px;">操作</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/07/15/2017:7:8/">随笔</a>
          </li>
        
          <li>
            <a href="/2017/07/15/tensorflow0/">tensorflow学习：神经网络基本概念形象理解</a>
          </li>
        
          <li>
            <a href="/2017/07/15/tensorflow1/">tensorflow学习： mnist 之softmax实现和CNN实现（1）</a>
          </li>
        
          <li>
            <a href="/2017/07/15/事件检测-paper/">用少量训练数据实现单张图片的事件检测</a>
          </li>
        
          <li>
            <a href="/2017/07/15/人体动作识别调研/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 guiyuliu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>